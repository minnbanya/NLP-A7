{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import os\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web crawler for AIT database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to crawl a webpage and extract links\n",
    "def crawl_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "        return links\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html_content):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove script tags and their contents\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "    \n",
    "    # Get text content\n",
    "    text_content = soup.get_text(separator=' ')\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text_content = ' '.join(text_content.split())\n",
    "    \n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_foreign_languages(text):\n",
    "    # Define a regex pattern to match non-ASCII characters\n",
    "    non_ascii_pattern = re.compile(r'[^\\x00-\\x7F]')  # Matches any character not in the ASCII range\n",
    "    \n",
    "    # Search for non-ASCII characters in the text\n",
    "    match = non_ascii_pattern.search(text)\n",
    "    \n",
    "    \n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error crawling http://giving.ait.ac.th/: No connection adapters were found for 'mailto:oaaa@ait.ac.th'\n",
      "Error crawling http://giving.ait.ac.th/2023/03/01/upgrading-of-the-ait-international-school-canteen/: Invalid URL '/2023/03/01/upgrading-of-the-ait-international-school-canteen/#respond': No scheme supplied. Perhaps you meant https:///2023/03/01/upgrading-of-the-ait-international-school-canteen/#respond?\n"
     ]
    }
   ],
   "source": [
    "# To check if crawled link is an AIT website\n",
    "ait_expr = r'ait'\n",
    "skip_expr = r'Just a moment... Enable JavaScript and cookies to continue|ï¿½|Rights Reserved. - Designed by Outsourcify|cookies|Log In|YouTube'\n",
    "\n",
    "# Start from AIT home page\n",
    "start_url = 'https://ait.ac.th'\n",
    "\n",
    "# Database\n",
    "# Create the 'AIT_database' folder if it doesn't exist\n",
    "database_folder = 'AIT_database'\n",
    "os.makedirs(database_folder, exist_ok=True)\n",
    "\n",
    "# Crawl the webpage\n",
    "crawled_data = []\n",
    "crawled_text = []\n",
    "links_to_crawl = [start_url]\n",
    "i = 0 # Counter\n",
    "while links_to_crawl and i < 100: # Limit to 100 links\n",
    "    current_url = links_to_crawl.pop(0)\n",
    "    try:\n",
    "        links = crawl_webpage(current_url)\n",
    "        for link in links:   \n",
    "            if link not in crawled_data and re.search(ait_expr, link):\n",
    "                text = requests.get(link).text\n",
    "                cleaned_text = clean_html(text)\n",
    "                if re.search(skip_expr, cleaned_text) is None and cleaned_text.strip() != '' and contains_foreign_languages(cleaned_text) and cleaned_text not in crawled_text:\n",
    "                    crawled_text.append(cleaned_text)\n",
    "                    crawled_data.append(f'Link: {link} \\n Document: {cleaned_text}')\n",
    "                    i += 1\n",
    "                    links_to_crawl.append(link)\n",
    "                    \n",
    "                    # Download PDF attachments\n",
    "                    attachments = re.findall(r'href=\"(.*?\\.pdf)\"', text)\n",
    "                    for attachment_url in attachments:\n",
    "                        attachment_response = requests.get(attachment_url)\n",
    "                        if attachment_response.status_code == 200:\n",
    "                            attachment_filename = os.path.basename(attachment_url)\n",
    "                            with open(os.path.join(database_folder, attachment_filename), 'wb') as f:\n",
    "                                f.write(attachment_response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {current_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save list of documents as PDF files\n",
    "def save_documents_as_pdf(documents, folder):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    for index, document in enumerate(documents, start=1):\n",
    "        filename = os.path.join(folder, f'Document_{index}.pdf')\n",
    "        doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "        story = [Paragraph(document, style=None)]\n",
    "        doc.build(story)\n",
    "\n",
    "# Example usage:\n",
    "# crawled_data = [...]  # List of documents\n",
    "save_documents_as_pdf(crawled_data, database_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
